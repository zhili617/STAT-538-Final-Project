\documentclass{article}[12pt]
%% narrow margin
% \oddsidemargin 1mm
% \evensidemargin 1mm
% \topmargin -30mm
% \textheight 700pt
% \textwidth 450pt

\oddsidemargin 2mm
\evensidemargin 2mm
\topmargin -15mm
\textheight 670pt
\textwidth 470pt
\oddsidemargin 3mm
\evensidemargin 3mm
\topmargin -12mm
\textheight 700pt
\textwidth 480pt

%typesetting
\usepackage{minted}
\usepackage{url}
\usepackage{hyperref}
\usepackage{color}
\usepackage{graphicx} % Required for inserting images
\usepackage{bbm}
\usepackage{float}
\usepackage{multirow,makeidx,algpseudocode,algorithm}
\usepackage{enumitem}
\usepackage{parskip}
\usepackage{setspace}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{caption}
% \usepackage{microtype}
\usepackage{appendix}
\usepackage{natbib}
% \usepackage[utf8]{inputenc} %archaic
\usepackage{verbatim}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  numbers=left,
  numberstyle=\tiny\color{gray},
  breaklines=true,
  backgroundcolor=\color{gray!10},
  frame=single
}

% \definecolor{codegreen}{rgb}{0,0.6,0}
% \definecolor{codegray}{rgb}{0.5,0.5,0.5}
% \definecolor{codepurple}{rgb}{0.58,0,0.82}
% \definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% \lstdefinestyle{mystyle}{
%     backgroundcolor=\color{backcolour},   
%     commentstyle=\color{codegreen},
%     keywordstyle=\color{magenta},
%     numberstyle=\tiny\color{codegray},
%     stringstyle=\color{codepurple},
%     basicstyle=\ttfamily\footnotesize,
%     breakatwhitespace=false,         
%     breaklines=true,                 
%     captionpos=b,                    
%     keepspaces=true,                 
%     numbers=left,                    
%     numbersep=5pt,                  
%     showspaces=false,                
%     showstringspaces=false,
%     showtabs=false,                  
%     tabsize=2
% }

% \lstset{style=mystyle}

%math
\usepackage{mathtools}
\usepackage{amsmath,amssymb,amsfonts,amsthm,amsbsy,amstext,mathrsfs}

\newcommand{\Fcal}{{\cal F}}
\newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Q}{\mathbb Q}
\newcommand{\N}{\mathbb N}
\newcommand{\C}{\mathbb{C}}
\newcommand{\1}{\mathbbm{1}}
\renewcommand{\P}{\mathbb P}
\newcommand{\E}{\mathbb E}
\newcommand{\Ecal}{{\cal E}}
\newcommand{\Acal}{{\cal A}}
\newcommand{\Mcal}{{\cal M}}
\newcommand{\Bcal}{{\cal B}}
\newcommand{\Scal}{{\cal S}}
\newcommand{\Xcal}{{\cal X}}
\newcommand{\eps}{\varepsilon}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Poi}{Poi}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Bernoulli}{Bernoulli}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\floor}[1]{\lfloor#1\rfloor}
\newcommand{\ceil}[1]{\lceil#1\rceil}
\newcommand{\ds}{\displaystyle}
\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\convdist}[0]{\overset{d}{\longrightarrow}}
\newcommand{\convprob}[0]{\overset{p}{\longrightarrow}}
\newcommand{\convas}[0]{\overset{a.s.}{\longrightarrow}}

% % reference
% \usepackage[round]{natbib}
% \bibliographystyle{abbrvnat}

\graphicspath{{./figures/}}


% Specifier	Meaning
% h	Try placing the float here (but only if it follows LaTeX’s spacing rules).
% t	Allow placement at the top of a page.
% b	Allow placement at the bottom of a page.
% p	Allow placement on a separate float-only page.
% H	Force exact placement (requires \usepackage{float}).
% !	Ignore some float placement restrictions to make placement more likely.



\begin{document}

\begin{titlepage}
    \centering
    {\LARGE \textbf{STAT538 Final Project} \par}\vspace{1cm}
    {\large \textbf{Understanding GLMM Fit Metrics: Comparing Binary, Poisson, and Zero-Inflated Models Across Cluster Designs} \par}
    \vspace{2cm}
    {\large Yingchi Guo\par}
    {Student Number: \par}
    {\large Sijia Li\par}
    {Student Number: \par}
    {\large Zhili Jiang\par}
    {Student Number:33635343 \par}
    \vspace{2cm}
    \today \par
    \vspace{5cm}
    \begin{abstract}
    R-squared measures are commonly used in linear models to quantify the proportion of variance in the outcome variable explained by the model. Although various R-squared measures have been proposed for linear mixed models (LMMs) and generalized linear models (GLMs), less attention has been paid to generalized linear mixed models (GLMMs) due to their added complexity—namely, non-normally distributed outcomes and nested data structures. Despite the growing emphasis on reporting R-squared as an effect size measure, researchers seldom report R-squared metrics for GLMMs in applied studies. In this paper, we build on the framework proposed by Nakagawa and Schielzeth (2013) to define an R-squared measure for GLMMs. We examine its conceptual relationship to existing R-squared measures for LMMs and GLMs, and conduct a set of simulations to compare pseudo R-squared metric with the Nakagawa and Schielzeth measure. Our aim is to clarify the use of R-squared in GLMMs and to identify practical considerations and future directions for its application.
\end{abstract}
    \vfill
\end{titlepage}
\newpage


\section{Introduction}

Generalized linear mixed models (GLMMs) have been widely used in fields such as economics, psychology, and education to analyze hierarchical data (i.e., observations nested within higher-level clusters) with non-normal dependent variables. For example, when predicting a binary dependent variable—such as whether an individual should be diagnosed with depressive disorder—there are often multiple behavioral observations (e.g., food intake, sleep duration) made for the same individual. In such cases, GLMMs are appropriate for modeling the nested structure of the data while accommodating the binary nature of the dependent variable.

With the increasing popularity of GLMMs comes a growing need to evaluate model fit, as an indicator of how well the specified model accounts for the variance in the data. Several information-based criteria have been proposed and are commonly used in practice, including the AIC, BIC, and DIC (in a Bayesian framework), as well as pseudo $R^2$ measures (e.g., McFadden’s pseudo $R^2$; @mcfadden1972) based on likelihood ratios. These information-based criteria are typically employed for model comparison using the same dataset, with the goal of selecting the best-fitting model. Meanwhile, McFadden’s pseudo $R^2$, as a likelihood-based measure, can also be used to evaluate the fit of a single model, interpreted as the degree of improvement over the intercept-only model.

Despite their widespread use, these measures have important limitations. As noted above, goodness-of-fit indices such as AIC, BIC, and DIC only assess relative model fit and do not provide information on how well the predictors explain variance in the dependent variable. Nor can they be used to compare results across studies [@claeskens2008; @nakagawa2007]. Additionally, neither goodness-of-fit measures nor likelihood-based indices offer insight into the proportion of outcome variance explained by the specified model [@orelien2008]. Given the increasing requirement from journals to report effect sizes, alternative measures—particularly $R^2$-based indices—should be considered.

Therefore, the purpose of this paper is to review a specific class of such measures: the $R^2$ indices proposed by @nakagawa2013. We aim to examine their effectiveness in quantifying the total outcome variance explained by the model at the population level. The remainder of this paper is organized as follows. We begin by reviewing the general structure of GLMMs, focusing on fixed-slope, random-intercept models. We then summarize existing $R^2$ measures for GLMs and LMMs separately, with special attention to the measures included in our simulation-based comparison. Next, we describe @nakagawa2013’s $R^2$ measures in detail and present simulation results comparing these measures with likelihood-based alternatives. We conclude with a discussion of future directions and practical recommendations for researchers conducting empirical studies.

\section{Generalized Linear Mixed Models}
Generalized linear mixed models (GLMMs) are characterized by the inclusion of random effects across higher-level clusters, and they are designed to handle non-normal dependent variables. For simplicity, we present a two-level, fixed-slope random-intercept model, where the slopes of predictors do not vary across clusters, but cluster-specific intercepts are included [@raudenbush2002; and see @johnson2014 for $R^2$ measures for random-slope models]. When the dependent variable $y_{ij}$ follows a distribution from the exponential family, a fixed-slope random-intercept GLMM model is written as:
$$
h(E(y_{ij})) = \mathbf{X}_{ij}\beta + u_j
\tag{1}
\label{eq:glmm}
$$

Here, the subscript $ij$ denotes the $i$th observation in the $j$th cluster. $\mathbf{X}_{ij}$ represents the predictor matrix (with the first column as 1s for the intercept), and $\beta$ denotes the fixed effects, including both intercept and slope coefficients. The random component, $u_j$, represents the cluster-specific random intercept (i.e., the deviation of the cluster-specific intercept from the overall fixed intercept). This term is also sometimes referred to as the level-2 residual, and it represents a key distinction between generalized linear models (GLMs) and GLMMs.

On the left-hand side, $h(\cdot)$ denotes the link function — a monotonic transformation that connects the expected value of $y_{ij}$ (on its original scale) to the linear predictor on the right-hand side of Equation \@ref(eq:glmm). Commonly used link functions include the logit link for binary outcomes and the log link for count outcomes, among others.

It is important to note that Equation \@ref(eq:glmm) does not explicitly include level-1 residuals. This is because, by definition, distributions in the exponential family have variances that are determined by their means (and, indirectly, by the link function). Therefore, the model is typically expressed with the level-1 residuals omitted, and the expected values appear within the link function. This formulation is essential for understanding the calculation and interpretation of $R^2$ measures in GLMMs (see sections below).

\section{Existing $R^2$ measures for GLM}
For GLMs, pseudo-$R^2$ measures are the most commonly reported goodness-of-fit metrics in applied research. However, none of these measures can be interpreted in the same way as the $R^2$ in linear regression models with normally distributed outcomes. Among the available options, a deviance-based measure and a likelihood-based measure are the most widely used; thus, we focus on these two here.

Originally described by @cohen2002, the deviance-based measure is given by:
$$
R^2_{Cohen} = \frac{D_{null} - D_{fitted}}{D_{null}}
\tag{2}
\label{eq:cohen}
$$

where $D_{null}$ and $D_{fitted}$ denote the deviance of the null model and the user-specified model, respectively. Equation \@ref(eq:cohen) represents the proportion of deviance reduced by the inclusion of predictors.

The most commonly used likelihood-based measure is McFadden’s pseudo-$R^2$, defined as:
$$
R^2_{McF} = 1 - \frac{\ln(L_{fitted})}{\ln(L_{null})}
\tag{3}
\label{eq:mcf}
$$

In this equation, $\ln(L_{fitted})$ and $\ln(L_{null})$ represent the log-likelihood of the fitted and null models, respectively. McFadden’s pseudo-$R^2$ is interpreted as the proportional improvement in log-likelihood achieved by including the predictors.

Both types of pseudo-$R^2$ can be readily extended to GLMMs, with the null model defined as a random-intercept-only model. We will conduct simulations to evaluate the effectiveness of the McFadden pseudo-$R^2$ measure in later sections.

\section{Existing $R^2$ measures for LMM}
With normally distributed outcomes, several outcome-variance-based $R^2$ measures have been proposed for linear mixed models (LMMs). Conceptually, $R^2$ is defined as:
$$
R^2 = \frac{\text{model-explained variance}}{\text{total outcome variance}}
\tag{4}
\label{eq:r2}
$$

Alternatively, it can be expressed as:
$$
R^2 = 1 - \frac{\text{residual variance}}{\text{total outcome variance}}
\tag{5}
\label{eq:r2resid}
$$

The definition in Equation \@ref(eq:r2) requires estimation of model-implied variances, while the definition in Equation \@ref(eq:r2resid) relies on residual variances.

In LMMs, however, one important consideration is whether level-2 residuals should be treated as part of the residual variance or as part of the model-implied variance. Both approaches are reasonable, which has led to two corresponding types of $R^2$: marginal $R^2$, where level-2 residuals are treated as residual variance, and conditional $R^2$, where level-2 residuals are considered part of the model effect.[^1]

[^1]: For models with random slopes, if the variance attributable to the random slopes is treated as residual, the result is a marginal $R^2$; if it is treated as part of the model structure, the result is a conditional $R^2$.

@raudenbush2002 proposed a variance-reduction approach for computing $R^2$ in LMMs, based on Equation \@ref(eq:r2resid):
$$
R^2_{\text{R\&B}} = 1 - \frac{\sum_{j=1}^J \sum_{i=1}^{n_j} (y_{ij} - \hat{y}_{ij})^2}
{\sum_{j=1}^J \sum_{i=1}^{n_j} (y_{ij} - \bar{y}_{ij})^2}
\tag{6}
\label{eq:rb}
$$

Here, $\bar{y}$ denotes the grand mean of the normally distributed outcome, $J$ is the number of clusters, and $n_j$ is the size of the $j$th cluster. The $\hat{y}_{ij}$ represents the model-predicted value for $y_{ij}$. Depending on whether $\hat{y}_{ij}$ is computed using only the fixed part of the model (i.e., $\mathbf{X}_{ij}\beta$, as in Equation \@ref(eq:glmm)), or both fixed and random parts (i.e., $\mathbf{X}_{ij}\beta + u_j$), the resulting measure corresponds to marginal or conditional $R^2$, respectively.

Based on the alternative definition of $R^2$ in Equation \@ref(eq:r2), @snijders2011 proposed a total variance decomposition approach:
$$
R^2_{\text{S\&B}} = 
\frac{\gamma_X^\prime \Sigma_X \gamma_X}
{\gamma_X^\prime \Sigma_X \gamma_X + \tau_{00} + \sigma^2}
\tag{7}
\label{eq:sb}
$$

Here, $\gamma_X$ is analogous to $\beta$ in Equation \@ref(eq:glmm), and $\Sigma_X$ is the covariance matrix of $\mathbf{X}_{ij}$. The term $\tau_{00}$ represents the variance of $u_j$, and $\sigma^2$ denotes the level-1 residual variance. This $R^2_{\text{S\&B}}$ can be interpreted as the proportion of total outcome variance explained by the fixed effects, including both intercept and slopes.

Similarly, @nakagawa2013 proposed conditional and marginal $R^2$ measures based on the same decomposition logic:
$$
R^2_{\text{Nakagawa}} = 
\frac{\sigma_f^2 + \sigma_l^2}
{\sigma_f^2 + \sigma_l^2 + \sigma_{\epsilon}^2}
\tag{8}
\label{eq:nakagawa}
$$

In this formulation, $\sigma_f^2$ represents the variance explained by the fixed effects, $\sigma_l^2$ represents the variance explained by the random effects, and $\sigma_{\epsilon}^2$ is the level-1 residual variance. This metric is interpreted as the proportion of total outcome variance explained jointly by both fixed and random effects. We will focus on this measure and discuss how it can be extended to the GLMM in subsequent sections.


\section{Simulation Design: Education Intervention Program}


The simulation represents an \textit{educational intervention evaluation plan} designed to assess the effectiveness of a distance learning course implemented across $100$ schools. The data-generating structure involves two levels of covariates:

\begin{itemize}
    \item \textbf{Individual-level variables} $\mathbf{X}_w = (X_{w1}, X_{w2})$, where $X_{w1}$ denotes \textit{Participation} and $X_{w2}$ denotes \textit{Pretest Score}.
    \item \textbf{School-level variables} $\mathbf{X}_b = (X_{b1}, X_{b2})$, where $X_{b1}$ represents \textit{School Funding} and $X_{b2}$ represents a \textit{Resource Index}.
\end{itemize}

Each school (cluster) contains a fixed number of students. Two experimental conditions define different class size scenarios:

\begin{itemize}
    \item \textbf{Small cluster scenario:} 5 students per school
    \item \textbf{Large cluster scenario:} 50 students per school
\end{itemize}

Unobserved heterogeneity across schools is modeled using \textit{random intercepts}:
\[
u_j \sim \mathcal{N}(0, \tau^2)
\]

Under this hierarchical framework, the simulation investigates four outcome types using generalized linear mixed models (GLMMs) and their zero-inflated extensions. Each outcome corresponds to a distinct research question.


\subsection{Binary GLMM}
Did the intervention course increase the probability of students passing the final assessment (0/1)?

\textbf{Corresponding Variable}: Whether the course is passed (\texttt{y = 1} passed, \texttt{y = 0} failed)
\begin{align*}
    \text{logit}(p_{ij}) = \beta_0 + \beta_1 X_{w1,ij} + \beta_2 X_{w2,ij} + \beta_3 X_{b1,j} + \beta_4 X_{b2,j} + u_{j}
\end{align*}

where $i$ refers to the ith student and $j$ refers to the jth school.

\subsection{Poisson GLMM}
How many Self-Study Modules do students complete during the course?

\textbf{Corresponding Variable}: The number of completed Modules.

\begin{align*}
    \log(\lambda_{ij}) = \beta_0 + \beta_1 X_{w1,ij} + \beta_2 X_{w2,ij} + \beta_3 X_{b1,j} + \beta_4 X_{b2,j} + u_{j}
\end{align*}

\subsection{Zero-inflated GLMM}

We designed a zero-inflated binary data-generating mechanism in which the probability of a structural zero depends on baseline risk and two binary covariates: network quality and student health status. Specifically:

\begin{itemize}
    \item \textbf{$Z_{a1, ij}$}: continuous network quality for individual $i$ in cluster $j$.

    \item \textbf{$Z_{a2, ij}$}:  binary indicator of sick status (1 = sick, 0 = health).
    
    \item The baseline structural zero probability is set to $10\%$ for students with good network access and good health

    \item An additional $20\%$ is added to the structural zero probability for students with poor health status.

    \item A unit decrease in network quality leads to an approximate $15\%$ increase in structural zero probability;
\end{itemize}

The structural zero probability $\pi_{ij}$ is modeled using a logstic function
\begin{align*}
    \text{logit}(\pi_{ij}) = \gamma_0 + \gamma_1 Z_{a1, ij} + \gamma_2 Z_{a2, ij}
\end{align*}

\begin{table}[ht]
    \centering
    \caption{Multiplicative Overdispersion Model}
    \label{tab:simulated_bf}
    \begin{threeparttable}
        \begin{tabular}{@{}p{0.5\linewidth} p{0.5\linewidth}@{}}
            \toprule
            \multicolumn{1}{c}{Binomial Data} & \multicolumn{1}{c}{Count Data} \\
            \midrule
            a & b \\
            \begin{minipage}[t]{\linewidth}
    $Y_{ij} \sim \text{Overdispersed Binomial}(m_{ij}, p_{ij}, \omega)$ \\
    $\log\left(\frac{p_{ij}}{1 - p_{ij}}\right) = \beta_0 + \alpha_i$ \\
    $\alpha_i \sim N(0, \sigma^2_{\alpha})$
            \end{minipage} & 
            \begin{minipage}[t]{\linewidth}
    $Y_{ij} \sim \text{Overdispersed Poisson}(\mu_{ij}, \omega)$ \\
    $\mu_{ij}=exp(\beta_0 + \alpha_i)$ \\
    $\alpha_i \sim N(0, \sigma^2_{\alpha})$
            \end{minipage} \\
            & \\
            \begin{minipage}[t]{\linewidth}
                $Y_{ij}$: For the $i^{th}$ individual in the $j^{th}$ group, number of successes in $m_{ij}$ trials. \\
                For binary data, $m_{ij}=1$. \\
                $\alpha_i$: Random effect with variance $\sigma_{\alpha}^2$. \\
                $\omega$: Overdispersion parameter. \\
            \end{minipage}
            &
            \begin{minipage}[t]{\linewidth}
                $Y_{ij}$: For the $i^{th}$ individual in the $j^{th}$ group, the observed number of counts. \\
                $\mu_i$: Latent mean for the $i^{th}$ individual in the $j^{th}$ group. \\
                $\alpha_i$: Random effect with variance $\sigma_{\alpha}^2$. \\
                $\omega$: Overdispersion parameter. 
            \end{minipage} \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \small
            \item \textit{Note.} Notations can be found in Sections 3.1 and 3.2. NA = do not measure the reliability of any weighted linear composites. 
        \end{tablenotes}
    \end{threeparttable}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Additive Overdispersion Model}
    \label{tab:simulated_bf}
    \begin{threeparttable}
        \begin{tabular}{@{}p{0.5\linewidth} p{0.5\linewidth}@{}}
            \toprule
            \multicolumn{1}{c}{Binomial Data} & \multicolumn{1}{c}{Count Data} \\
            \midrule
            a & b \\
            \begin{minipage}[t]{\linewidth}
    $Y_{ij} \sim \text{Binomial}(m_{ij}, p_{ij})$ \\
    $\log\left(\frac{p_{ij}}{1 - p_{ij}}\right) = \beta_0 + \alpha_i + e_{ij}$ \\
    $\alpha_i \sim N(0, \sigma^2_{\alpha})$ \\
    $e_{ij} \sim N(0, \sigma_{e}^2)$
            \end{minipage} & 
            \begin{minipage}[t]{\linewidth}
    $Y_{ij} \sim \text{Poisson}(\mu_{ij})$ \\
    $\mu_{ij}=exp(\beta_0 + \alpha_i + e_{ij})$ \\
    $\alpha_i \sim N(0, \sigma^2_{\alpha})$ \\
    $e_{ij} \sim N(0, \sigma_{e}^2)$
            \end{minipage} \\
            & \\
            \begin{minipage}[t]{\linewidth}
                $Y_{ij}$: For the $i^{th}$ individual in the $j^{th}$ group, number of successes in $m_{ij}$ trials. \\
                For binary data, $m_{ij}=1$. \\
                $\alpha_i$: Random effect with variance $\sigma_{\alpha}^2$. \\
                $e_{ij}$: additive overdispersion (residual) term on the link scale, with a variance of $\sigma_e^2$. When $\sigma_e^2=0$, the model reduces to the normal binomial GLMM. 
            \end{minipage}
            &
            \begin{minipage}[t]{\linewidth}
                $Y_{ij}$: For the $i^{th}$ individual in the $j^{th}$ group, the observed number of counts. \\
                $\mu_i$: Latent mean for the $i^{th}$ individual in the $j^{th}$ group. \\
                $\alpha_i$: Random effect with variance $\sigma_{\alpha}^2$. \\
                $e_{ij}$: additive overdispersion (residual) term on the link scale, with a variance of $\sigma_e^2$. When $\sigma_e^2=0$, the model reduces to the normal Poisson GLMM. 
            \end{minipage} \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \small
            \item \textit{Note.} Notations can be found in Sections 3.1 and 3.2. NA = do not measure the reliability of any weighted linear composites. 
        \end{tablenotes}
    \end{threeparttable}
\end{table}


\subsubsection{Zero-inflated Binary GLMM}
There are some students who fail the course because they never take the test.


\[
P(Y_{ij} = y) = 
\begin{cases}
\pi_{ij} + (1 - \pi_{ij})(1 - p_{ij}), & \text{if } y = 0 \\
(1 - \pi_{ij}) p_{ij}, & \text{if } y = 1
\end{cases}
\]

\subsubsection{Zero-inflated Poisson GLMM}

There are some students who do not complete any module due to sickness and bad internet.

\[
P(Y = y) = 
\begin{cases}
\pi_{ij} + (1 - \pi_{ij})e^{-\lambda_{ij}}, & \text{if } y = 0 \\
(1 - \pi_{ij}) \dfrac{e^{-\lambda_{ij}} \lambda_{ij}^y}{y!}, & \text{if } y > 0
\end{cases}
\]

\section{Likelihood}

\section{Raudenbush \& Bryk $R^2$}

\section{Proportional Change in Variance}


\section{Nakagawa \& Schielzeth's $R^2$}

\subsection{Marginal $R^2$}

\subsection{Conditional $R^2$}

\begin{table}[ht]
    \centering
    \caption{Marginal and Conditional R-squared for GLMM: Multiplicative Overdispersion Model}
    \label{tab:simulated_bf}
    \begin{threeparttable}
        \begin{tabular}{@{}p{0.2\linewidth} p{0.5\linewidth} p{0.5\linewidth} @{}}
            \toprule
            \multicolumn{1}{c}{Data} & \multicolumn{1}{c}{Marginal $R^2$} & \multicolumn{1}{c}{Conditional $R^2$} \\
            \midrule
            & \begin{minipage}[t]{\linewidth}
    $R^2_{GLMM(m)} = \frac{\sigma^2_f}{\sigma_{f}^2 + \sum_{l=1}^u \sigma_l^2 + \omega \sigma_d^2}$ \\
    $\sigma_{\epsilon}^2 = \omega \sigma_d^2$
            \end{minipage} & 
            \begin{minipage}[t]{\linewidth}
    $R^2_{GLMM(c)} = \frac{\sigma^2_f + \sum_{l=1}^u \sigma_l^2}{\sigma_{f}^2 + \sum_{l=1}^u \sigma_l^2 + \omega \sigma_d^2}$ \\
    $\sigma_{\epsilon}^2 = \omega \sigma_d^2$
            \end{minipage} \\
            & \\
             &
\multicolumn{2}{l}{
    \begin{minipage}[t]{\linewidth}
        $\sigma_{f}^2$: Variance explained by the fixed effect components. \\
        $\sigma_{l}^2$: Variance component of the $j^{th}$ random factor. \\
        $\sigma_{d}^2$: Distribution-specific variance. \\
    \end{minipage}
} \\
Binomial Data & \multicolumn{2}{l}{$\sigma_d^2 = \pi^2/3$} \\
Count Data & \multicolumn{2}{l}{$\sigma_d^2 = \ln (1/\text{exp}(\beta_0) + 1)$} \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \small
            \item \textit{Note.} Notations can be found in Sections 3.1 and 3.2. NA = do not measure the reliability of any weighted linear composites. 
        \end{tablenotes}
    \end{threeparttable}
\end{table}


\begin{table}[ht]
    \centering
    \caption{Marginal and Conditional R-squared for GLMM: Additive Overdispersion Model}
    \label{tab:simulated_bf}
    \begin{threeparttable}
        \begin{tabular}{@{}p{0.2\linewidth} p{0.5\linewidth} p{0.5\linewidth} @{}}
            \toprule
            \multicolumn{1}{c}{Data} & \multicolumn{1}{c}{Marginal $R^2$} & \multicolumn{1}{c}{Conditional $R^2$} \\
            \midrule
            & \begin{minipage}[t]{\linewidth}
    $R^2_{GLMM(m)} = \frac{\sigma^2_f}{\sigma_{f}^2 + \sum_{l=1}^u \sigma_l^2 + \sigma_e^2 + \sigma_d^2}$ \\
    $\sigma_{\epsilon}^2 = \sigma_e^2 + \sigma_d^2$
            \end{minipage} & 
            \begin{minipage}[t]{\linewidth}
    $R^2_{GLMM(c)} = \frac{\sigma^2_f + \sum_{l=1}^u \sigma_l^2}{\sigma_{f}^2 + \sum_{l=1}^u \sigma_l^2 + \sigma_e^2 + \sigma_d^2}$ \\
    $\sigma_{\epsilon}^2 = \sigma_e^2 + \sigma_d^2$
            \end{minipage} \\
            & \\
             &
\multicolumn{2}{l}{
    \begin{minipage}[t]{\linewidth}
        $\sigma_{f}^2$: Variance explained by the fixed effect components. \\
        $\sigma_{l}^2$: Variance component of the $j^{th}$ random factor. \\
        $\sigma_{e}^2$: Variance of additive overdispersion residual term on the link scale. \\
        $\sigma_{d}^2$: Distribution-specific variance. \\
    \end{minipage}
} \\
Binomial Data & \multicolumn{2}{l}{$\sigma_d^2 = \pi^2/3$} \\
Count Data & \multicolumn{2}{l}{$\sigma_d^2 = \ln (1/\text{exp}(\beta_0) + 1)$} \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \small
            \item \textit{Note.} Notations can be found in Sections 3.1 and 3.2. NA = do not measure the reliability of any weighted linear composites. 
        \end{tablenotes}
    \end{threeparttable}
\end{table}

\section{Conlcusion}

\nocite{*}
\bibliographystyle{apalike}  
\bibliography{ref}


\clearpage
\appendix
\begin{center}
{\LARGE \textbf{Appendix}}  
\end{center}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\setcounter{figure}{0}  % Reset figure counter
\setcounter{table}{0}   % Reset table counter


\end{document}
