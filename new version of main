\documentclass{article}[12pt]
%% narrow margin
% \oddsidemargin 1mm
% \evensidemargin 1mm
% \topmargin -30mm
% \textheight 700pt
% \textwidth 450pt

\oddsidemargin 2mm
\evensidemargin 2mm
\topmargin -15mm
\textheight 670pt
\textwidth 470pt
\oddsidemargin 3mm
\evensidemargin 3mm
\topmargin -12mm
\textheight 700pt
\textwidth 480pt

%typesetting
\usepackage{amsmath}
\usepackage{minted}
\usepackage{url}
\usepackage{hyperref}
\usepackage{color}
\usepackage{graphicx} % Required for inserting images
\usepackage{bbm}
\usepackage{float}
\usepackage{multirow,makeidx,algpseudocode,algorithm}
\usepackage{enumitem}
\usepackage{parskip}
\usepackage{setspace}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{pdflscape}
\usepackage{rotating} 
\usepackage{booktabs}
% \usepackage{microtype}
\usepackage{appendix}
\usepackage{natbib}
% \usepackage[utf8]{inputenc} %archaic
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  numbers=left,
  numberstyle=\tiny\color{gray},
  breaklines=true,
  backgroundcolor=\color{gray!10},
  frame=single
}

% \definecolor{codegreen}{rgb}{0,0.6,0}
% \definecolor{codegray}{rgb}{0.5,0.5,0.5}
% \definecolor{codepurple}{rgb}{0.58,0,0.82}
% \definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% \lstdefinestyle{mystyle}{
%     backgroundcolor=\color{backcolour},   
%     commentstyle=\color{codegreen},
%     keywordstyle=\color{magenta},
%     numberstyle=\tiny\color{codegray},
%     stringstyle=\color{codepurple},
%     basicstyle=\ttfamily\footnotesize,
%     breakatwhitespace=false,         
%     breaklines=true,                 
%     captionpos=b,                    
%     keepspaces=true,                 
%     numbers=left,                    
%     numbersep=5pt,                  
%     showspaces=false,                
%     showstringspaces=false,
%     showtabs=false,                  
%     tabsize=2
% }

% \lstset{style=mystyle}

%math
\usepackage{mathtools}
\usepackage{amsmath,amssymb,amsfonts,amsthm,amsbsy,amstext,mathrsfs}

\newcommand{\Fcal}{{\cal F}}
\newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Q}{\mathbb Q}
\newcommand{\N}{\mathbb N}
\newcommand{\C}{\mathbb{C}}
\newcommand{\1}{\mathbbm{1}}
\renewcommand{\P}{\mathbb P}
\newcommand{\E}{\mathbb E}
\newcommand{\Ecal}{{\cal E}}
\newcommand{\Acal}{{\cal A}}
\newcommand{\Mcal}{{\cal M}}
\newcommand{\Bcal}{{\cal B}}
\newcommand{\Scal}{{\cal S}}
\newcommand{\Xcal}{{\cal X}}
\newcommand{\eps}{\varepsilon}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Poi}{Poi}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Bernoulli}{Bernoulli}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\floor}[1]{\lfloor#1\rfloor}
\newcommand{\ceil}[1]{\lceil#1\rceil}
\newcommand{\ds}{\displaystyle}
\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\convdist}[0]{\overset{d}{\longrightarrow}}
\newcommand{\convprob}[0]{\overset{p}{\longrightarrow}}
\newcommand{\convas}[0]{\overset{a.s.}{\longrightarrow}}

% % reference
% \usepackage[round]{natbib}
% \bibliographystyle{abbrvnat}

\graphicspath{{./figures/}}


% Specifier	Meaning
% h	Try placing the float here (but only if it follows LaTeX’s spacing rules).
% t	Allow placement at the top of a page.
% b	Allow placement at the bottom of a page.
% p	Allow placement on a separate float-only page.
% H	Force exact placement (requires \usepackage{float}).
% !	Ignore some float placement restrictions to make placement more likely.



\begin{document}

\begin{titlepage}
    \centering
    {\LARGE \textbf{STAT538 Final Project} \par}\vspace{1cm}
    {\large \textbf{Understanding GLMM Fit Metrics: Comparing Binary, Poisson, and Zero-Inflated Models Across Cluster Designs} \par}
    \vspace{2cm}
    {\large Yingchi Guo\par}
    {\large Sijia Li\par}
    {\large Zhili Jiang\par}
    \vspace{2cm}
    \today \par
    \vspace{5cm}
    \begin{abstract}
    R-squared measures are commonly used in linear models to quantify the proportion of variance in the outcome variable explained by the model. Although various R-squared measures have been proposed for linear mixed models (LMMs) and generalized linear models (GLMs), less attention has been paid to generalized linear mixed models (GLMMs) due to their added complexity—namely, non-normally distributed outcomes and nested data structures. Despite the growing emphasis on reporting R-squared as an effect size measure, researchers seldom report R-squared metrics for GLMMs in applied studies. In this paper, we build on the framework proposed by Nakagawa and Schielzeth (2013) to define an R-squared measure for GLMMs. We examine its conceptual relationship to existing R-squared measures for LMMs and GLMs, and conduct a set of simulations to compare pseudo R-squared metric with the Nakagawa and Schielzeth measure. Our aim is to clarify the use of R-squared in GLMMs and to identify practical considerations and future directions for its application.
\end{abstract}
    \vfill
\end{titlepage}
\newpage


\section{Introduction}

Generalized linear mixed models (GLMMs) have been widely used in fields such as economics, psychology, and education to analyze hierarchical data (i.e., observations nested within higher-level clusters) with non-normal dependent variables. For example, when predicting a binary dependent variable—such as whether an individual should be diagnosed with depressive disorder—there are often multiple behavioral observations (e.g., food intake, sleep duration) made for the same individual. In such cases, GLMMs are appropriate for modeling the nested structure of the data while accommodating the binary nature of the dependent variable.

With the increasing popularity of GLMMs comes a growing need to evaluate model fit, as an indicator of how well the specified model accounts for the variance in the data. Several information-based criteria have been proposed and are commonly used in practice, including the AIC, BIC, and DIC (in a Bayesian framework), as well as pseudo $R^2$ measures (e.g., McFadden’s pseudo $R^2$; \citet{mcfadden1972}) based on likelihood ratios. These information-based criteria are typically employed for model comparison using the same dataset, with the goal of selecting the best-fitting model. Meanwhile, McFadden’s pseudo $R^2$, as a likelihood-based measure, can also be used to evaluate the fit of a single model, interpreted as the degree of improvement over the intercept-only model.

Despite their widespread use, these measures have important limitations. As noted above, goodness-of-fit indices such as AIC, BIC, and DIC only assess relative model fit and do not provide information on how well the predictors explain variance in the dependent variable. Nor can they be used to compare results across studies [\citet{claeskens2008}; \citet{nakagawa2007}]. Additionally, neither goodness-of-fit measures nor likelihood-based indices offer insight into the proportion of outcome variance explained by the specified model [\citet{orelien2008}]. Given the increasing requirement from journals to report effect sizes, alternative measures—particularly $R^2$-based indices—should be considered.

Therefore, the purpose of this paper is to review a specific class of such measures: the $R^2$ indices proposed by \citet{nakagawa2013}. We aim to examine their effectiveness in quantifying the total outcome variance explained by the model at the population level. The remainder of this paper is organized as follows. We begin by reviewing the general structure of GLMMs, focusing on fixed-slope, random-intercept models. We then summarize existing $R^2$ measures for GLMs and LMMs separately, with special attention to the measures included in our simulation-based comparison. Next, we describe \citet{nakagawa2013}’s $R^2$ measures in detail and present simulation results comparing these measures with likelihood-based alternatives. We conclude with a discussion of future directions and practical recommendations for researchers conducting empirical studies.

\section{Generalized Linear Mixed Models}
Generalized linear mixed models (GLMMs) are characterized by the inclusion of random effects across higher-level clusters, and they are designed to handle non-normal dependent variables. For simplicity, we present a two-level, fixed-slope random-intercept model, where the slopes of predictors do not vary across clusters, but cluster-specific intercepts are included [\citet{raudenbush2002}; and see \citet{johnson2014} for $R^2$ measures for random-slope models]. When the dependent variable $y_{ij}$ follows a distribution from the exponential family, a fixed-slope random-intercept GLMM model is written as:
\begin{equation}
h(E(y_{ij})) = \mathbf{X}_{ij} \beta + u_j
\tag{1}
\label{eq:glmm}
\end{equation}


Here, the subscript $ij$ denotes the $i$th observation in the $j$th cluster. $\mathbf{X}_{ij}$ represents the predictor matrix (with the first column as 1s for the intercept), and $\beta$ denotes the fixed effects, including both intercept and slope coefficients. The random component, $u_j$, represents the cluster-specific random intercept (i.e., the deviation of the cluster-specific intercept from the overall fixed intercept). This term is also sometimes referred to as the level-2 residual, and it represents a key distinction between generalized linear models (GLMs) and GLMMs.

On the left-hand side, $h(\cdot)$ denotes the link function — a monotonic transformation that connects the expected value of $y_{ij}$ (on its original scale) to the linear predictor on the right-hand side of Equation \ref{eq:glmm}. Commonly used link functions include the logit link for binary outcomes and the log link for count outcomes, among others.

It is important to note that Equation \ref{eq:glmm} does not explicitly include level-1 residuals. This is because, by definition, distributions in the exponential family have variances that are determined by their means (and, indirectly, by the link function). Therefore, the model is typically expressed with the level-1 residuals omitted, and the expected values appear within the link function. This formulation is essential for understanding the calculation and interpretation of $R^2$ measures in GLMMs (see sections below).

\section{Existing $R^2$ measures for GLM}
For GLMs, pseudo-$R^2$ measures are the most commonly reported goodness-of-fit metrics in applied research. However, none of these measures can be interpreted in the same way as the $R^2$ in linear regression models with normally distributed outcomes. Among the available options, a deviance-based measure and a likelihood-based measure are the most widely used; thus, we focus on these two here.

Originally described by \citet{cohen2002}, the deviance-based measure is given by:
\begin{equation}
R^2_{Cohen} = \frac{D_{null} - D_{fitted}}{D_{null}}
\tag{2}
\label{eq:cohen}
\end{equation}

where $D_{null}$ and $D_{fitted}$ denote the deviance of the null model and the user-specified model, respectively. Equation \ref{eq:cohen} represents the proportion of deviance reduced by the inclusion of predictors.

The most commonly used likelihood-based measure is McFadden’s pseudo-$R^2$, defined as:
\begin{equation}
R^2_{McF} = 1 - \frac{\ln(L_{fitted})}{\ln(L_{null})}
\tag{3}
\label{eq:mcf}
\end{equation}

In this equation, $\ln(L_{fitted})$ and $\ln(L_{null})$ represent the log-likelihood of the fitted and null models, respectively. McFadden’s pseudo-$R^2$ is interpreted as the proportional improvement in log-likelihood achieved by including the predictors.

Both types of pseudo-$R^2$ can be readily extended to GLMMs, with the null model defined as a random-intercept-only model. We will conduct simulations to evaluate the effectiveness of the McFadden pseudo-$R^2$ measure in later sections.

\section{Existing $R^2$ measures for LMM}
With normally distributed outcomes, several outcome-variance-based $R^2$ measures have been proposed for linear mixed models (LMMs). Conceptually, $R^2$ is defined as:
\begin{equation}
R^2 = \frac{\text{model-explained variance}}{\text{total outcome variance}}
\tag{4}
\label{eq:r2}
\end{equation}

Alternatively, it can be expressed as:
\begin{equation}
R^2 = 1 - \frac{\text{residual variance}}{\text{total outcome variance}}
\tag{5}
\label{eq:r2resid}
\end{equation}

The definition in Equation \ref{eq:r2} requires estimation of model-implied variances, while the definition in Equation \ref{eq:r2resid} relies on residual variances.

In LMMs, however, one important consideration is whether level-2 residuals should be treated as part of the residual variance or as part of the model-implied variance. Both approaches are reasonable, which has led to two corresponding types of $R^2$: marginal $R^2$, where level-2 residuals are treated as residual variance, and conditional $R^2$, where level-2 residuals are considered part of the model effect.

\footnote{ For models with random slopes, if the variance attributable to the random slopes is treated as residual, the result is a marginal $R^2$; if it is treated as part of the model structure, the result is a conditional $R^2$.}

\citet{raudenbush2002} proposed a variance-reduction approach for computing $R^2$ in LMMs, based on Equation \ref{eq:r2resid}:
\begin{equation}
R^2_{\text{R\&B}} = 1 - \frac{\sum_{j=1}^J \sum_{i=1}^{n_j} (y_{ij} - \hat{y}_{ij})^2}
{\sum_{j=1}^J \sum_{i=1}^{n_j} (y_{ij} - \bar{y}_{ij})^2}
\tag{6}
\label{eq:rb}
\end{equation}

Here, $\bar{y}$ denotes the grand mean of the normally distributed outcome, $J$ is the number of clusters, and $n_j$ is the size of the $j$th cluster. The $\hat{y}_{ij}$ represents the model-predicted value for $y_{ij}$. Depending on whether $\hat{y}_{ij}$ is computed using only the fixed part of the model (i.e., $\mathbf{X}_{ij}\beta$, as in Equation \ref{eq:glmm}), or both fixed and random parts (i.e., $\mathbf{X}_{ij}\beta + u_j$), the resulting measure corresponds to marginal or conditional $R^2$, respectively.

Based on the alternative definition of $R^2$ in Equation \ref{eq:r2}, \citet{snijders2011} proposed a total variance decomposition approach:
\begin{equation}
R^2_{\text{S\&B}} = 
\frac{\gamma_X^\prime \Sigma_X \gamma_X}
{\gamma_X^\prime \Sigma_X \gamma_X + \tau_{00} + \sigma^2}
\tag{7}
\label{eq:sb}
\end{equation}

Here, $\gamma_X$ is analogous to $\beta$ in Equation \ref{eq:glmm}, and $\Sigma_X$ is the covariance matrix of $\mathbf{X}_{ij}$. The term $\tau_{00}$ represents the variance of $u_j$, and $\sigma^2$ denotes the level-1 residual variance. This $R^2_{\text{S\&B}}$ can be interpreted as the proportion of total outcome variance explained by the fixed effects, including both intercept and slopes.

Similarly, \citet{nakagawa2013} proposed conditional and marginal $R^2$ measures based on the same decomposition logic:
\begin{equation}
R^2_{\text{Nakagawa}} = 
\frac{\sigma_f^2 + \sigma_l^2}
{\sigma_f^2 + \sigma_l^2 + \sigma_{\epsilon}^2}
\tag{8}
\label{eq:nakagawa}
\end{equation}

In this formulation, $\sigma_f^2$ represents the variance explained by the fixed effects, $\sigma_l^2$ represents the variance explained by the random effects, and $\sigma_{\epsilon}^2$ is the level-1 residual variance. This metric is interpreted as the proportion of total outcome variance explained jointly by both fixed and random effects. We will focus on this measure and discuss how it can be extended to the GLMM in subsequent sections.







\section{Likelihood-based $R^2$ Measures for GLMM}

\subsection{McFadden $R^2$}

\subsection{Raudenbush \& Bryk $R^2$}

\subsection{Proportional Change in Variance}
Proportional Change in Variance (PCV) quantifies how the variance of random effects changes after introducing additional fixed-effect predictors. It helps assess whether a predictor explains between-group variability (e.g., school-level differences) or within-group variability (e.g., individual differences). The formula is given by:

\begin{align*}
    \text{PCV} = \frac{\hat{\sigma}^2_{u,\text{null}} - \hat{\sigma}^2_{u,\text{model}}}{\hat{\sigma}^2_{u,\text{null}}}
\end{align*}

where $\hat{\sigma}^2_{u,\text{null}}$ denotes the variance of the random intercept in the null model, and $\hat{\sigma}^2_{u,\text{model}}$ denotes the variance in the full model that includes additional predictors.



\section{Nakagawa \& Schielzeth's $R^2$}

In GLMM, there is no direct estimate of residual variance $\sigma_e^2$. Since data is modeled through a link function in GLMM (e.g., logit link, probit link, log link, etc.), an alternative way is to consider residual variance as the variance not captured by the link function. \citet{nakagawa2013} proposed that the residual variance can be decomposed into three components: (1) the latent link-scale distribution variance, (2) the overdispersion parameter obtained from the dataset, representing the variance not captured by the distribution-specific variance, and (3) a link-scale residual variance. Two models are employed for calculating these components: a multiplicative overdispersion model and an additive overdispersion model.  In this section, we first define the multiplicative overdispersion model and additive overdispersion model. Second, we define marginal and conditional $R^2$ based on these models, respectively. 

\subsection{Multiplicative and Additive Overdispersion Model}
In a multiplicative model, the overdipersion is modeled through an additional parameter in the distribution of the response variable. A simple GLMM for binomial data can be written as: 
\begin{align*}
    Y_{ij} \sim \text{Overdispersed Binomial}(m_{ij}, p_{ij}, \omega) \\
    \log\left(\frac{p_{ij}}{1 - p_{ij}}\right) = \beta_0 + \alpha_i \\
    \alpha_i \sim N(0, \sigma^2_{\alpha})
\end{align*}
where $Y_{ij}$ is the number of success, $m_{ij}$ is the number of trials, $p_{ij}$ is the probability of success, for the $i^{th}$ individual in the $j^{th}$ group, $\omega$ is the overdispersion parameter, $\sigma_{\alpha}^2$ is the between-person variance, $\alpha_i$ is the normally distributed between-person random effect. For binary data $m_{ij}=1$. When $\omega=1$, there is no overdispersion in the data, and $Y_{ij}$ follows a normal binomial distribution. 

A simple GLMM for count data can be written as:
\begin{align*}
    Y_{ij} \sim \text{Overdispersed Poisson}(\mu_{ij}, \omega) \\
    \mu_{ij}=exp(\beta_0 + \alpha_i) \\
    \alpha_i \sim N(0, \sigma^2_{\alpha})
\end{align*} where $\mu_{ij}$ is the mean of the Poisson distribution for the $i^{th}$ individual in the $j^{th}$ group. When $\omega=1$, there is no overdispersion in the data, and $Y_{ij}$ follows a normal Poisson distribution.

In both cases mentioned above, $\sigma_{\epsilon}^2 = \omega \sigma_{d}^2$, where $\sigma_d^2$ is the variance of the link-scale distribution. For binomial data, $\sigma_d^2$ is the variance of the logistic distribution, let $d_l \sim \text{Logistic}(0,1)$, that is:
\begin{align*}
    \sigma_d^2 = E[d_l^2] - (E[d_l])^2 = \int_{-\infty}^{\infty} d_l^2 \cdot \frac{e^{-d_l}}{(1 + e^{-d_l})^2} \, dx  - 0 = \frac{\pi^2}{3}
\end{align*} For Poisson data, $\sigma_d^2$ is the variance of the variance of the lognormal approximation for a Poisson distribution, let $d_p \sim \text{Log$N$($\mu, \sigma^2$)}$ to match a Poisson distribution with mean and variance of $\lambda$: \begin{align*}
    E[d_p] = e^{\mu + \sigma^2/2} = \lambda, &\quad \mu = ln(\lambda) - \frac{\sigma^2}{2} \\
    Var(d_p) = (e^{\sigma^2}-1)e^{2\mu + \sigma^2} = \lambda, &\quad (e^{\sigma^2}-1)\lambda^2 = \lambda, \quad \sigma^2 = ln(1 + \frac{1}{\lambda})
\end{align*} Therefore, $\sigma_d^2 = ln(1 + \frac{1}{\lambda})$ (see also \citet{nakagawa2017r2icc}). Table~\ref{tab:simulated_bf_mom} presents a summary of multiplicative models, for binomial and count data.  

In an additive model, the overdispersion is modeled through a residual term on the latent link scale (\citet{browne2005variance}). A simple GLMM for binomial data can be written as:
\begin{align*}
    Y_{ij} \sim \text{Binomial}(m_{ij}, p_{ij}) \\
    \log\left(\frac{p_{ij}}{1 - p_{ij}}\right) = \beta_0 + \alpha_i + e_{ij} \\
    \alpha_i \sim N(0, \sigma^2_{\alpha}) \\
    e_{ij} \sim N(0, \sigma_{e}^2)
\end{align*} where $e_{ij}$ is the normally distributed residual term on the log scale, and $\sigma_e^2$ is the latent link-scale residual variance. When $\sigma_e^2=0$, the model reduces to the normal binomial GLMM. 

A simple GLMM for count data can be written as:
\begin{align*}
    Y_{ij} \sim \text{Poisson}(\mu_{ij}) \\
    \mu_{ij}=exp(\beta_0 + \alpha_i + e_{ij}) \\
    \alpha_i \sim N(0, \sigma^2_{\alpha}) \\
    e_{ij} \sim N(0, \sigma_{e}^2)
\end{align*}  When $\sigma_e^2=0$, the model reduces to the normal Poisson GLMM. In both cases mentioned above, the model residual variance $\sigma_{\epsilon}^2 = \sigma_e^2 + \sigma_d^2$. Table~\ref{tab:simulated_bf_aom} presents a summary of additive model, for binomial and count data.

\subsection{Conditional and Marginal $R^2$}

For GLMM, two types of $R^2$ are usually considered: the marginal $R^2$ and conditional $R^2$. The marginal $R^2$ measures the proportion of variance in the response variable explained by the fixed effects in the model. The conditional $R^2$ measures the proportion of variance in the response variable explain by the fixed effects and random effects in the model. Based on the multiplicative and additive overdispersion model defined above, the conditional and marginal $R^2$ of GLMM can be defined accordingly. 

Based on the multiplicative model, the marginal and conditional $R^2$ are: 
\begin{equation*}
    R^2_{GLMM(m)} = \frac{\sigma^2_f}{\sigma_{f}^2 + \sum_{l=1}^u \sigma_l^2 + \omega \sigma_d^2}, \quad R^2_{GLMM(c)} = \frac{\sigma^2_f + \sum_{l=1}^u \sigma_l^2}{\sigma_{f}^2 + \sum_{l=1}^u \sigma_l^2 + \omega \sigma_d^2}
\end{equation*}
where $m$ in the parentheses indicates marginal $R^2$, $c$ in the parentheses indicates conditional $R^2$, $\sigma_f^2$ indicates the variance explained by the fixed effects, $\sigma_l^2$ indicates the variance explained by the random effects. 

Based on the additive model, the marginal and conditional $R^2$ are:
\begin{equation*}
    R^2_{GLMM(m)} = \frac{\sigma^2_f}{\sigma_{f}^2 + \sum_{l=1}^u \sigma_l^2 + \sigma_e^2 + \sigma_d^2}, \quad R^2_{GLMM(c)} = \frac{\sigma^2_f + \sum_{l=1}^u \sigma_l^2}{\sigma_{f}^2 + \sum_{l=1}^u \sigma_l^2 + \sigma_e^2 + \sigma_d^2} 
\end{equation*} Table~\ref{tab:simulated_bf_multiplicative} and~\ref{tab:simulated_bf_additive} summarizes these formulae and presents specific formulae for binomial and count data.

\section{Simulation Design: Education Intervention Program}


The simulation represents an \textit{educational intervention evaluation plan} designed to assess the effectiveness of a distance learning course implemented across $100$ schools. The data-generating structure involves two levels of covariates:

\begin{itemize}
    \item \textbf{Individual-level variables} $\mathbf{X}_w = (X_{w1}, X_{w2})$, where $X_{w1}$ denotes \textit{Participation} and $X_{w2}$ denotes \textit{Pretest Score}.
\end{itemize}

Each school (cluster) contains a fixed number of students. Two experimental conditions define different class size scenarios:

\begin{itemize}
    \item \textbf{Small cluster scenario:} 5 students per school
    \item \textbf{Large cluster scenario:} 50 students per school
\end{itemize}

Unobserved heterogeneity across schools is modeled using \textit{random intercepts}:
\[
u_j \sim \mathcal{N}(0, \tau^2)
\]

Under this hierarchical framework, the simulation investigates four outcome types using generalized linear mixed models (GLMMs) and their zero-inflated extensions. Each outcome corresponds to a distinct research question.


\subsection{Binary GLMM}
Did the intervention course increase the probability of students passing the final assessment (0/1)?

\textbf{Corresponding Variable}: Whether the course is passed (\texttt{y = 1} passed, \texttt{y = 0} failed)
\begin{align*}
    \text{logit}(p_{ij}) = \beta_0 + \beta_1 X_{w1,ij} + \beta_2 X_{w2,ij}  + u_{j}
\end{align*}

where $i$ refers to the ith student and $j$ refers to the jth school.

\subsection{Poisson GLMM}
How many Self-Study Modules do students complete during the course?

\textbf{Corresponding Variable}: The number of completed Modules.

\begin{align*}
    \log(\lambda_{ij}) = \beta_0 + \beta_1 X_{w1,ij} + \beta_2 X_{w2,ij}+ u_{j}
\end{align*}

\subsection{Zero-inflated GLMM}

We designed a zero-inflated binary data-generating mechanism in which the probability of a structural zero depends on baseline risk and two binary covariates: network quality and student health status. Specifically:

\begin{itemize}
    \item \textbf{$Z_{a1, ij}$}: continuous network quality for individual $i$ in cluster $j$, using $N(0, 1)$ to simulate.

    \item \textbf{$Z_{a2, ij}$}:  binary indicator of sick status (1 = sick, 0 = health), using $Bin(0.1)$ to simulate.
    
    \item The baseline structural zero probability is set to $10\%$ for students with good network access and good health

    \item An additional $20\%$ is added to the structural zero probability for students with poor health status.

    \item A unit decrease in network quality leads to an approximate $15\%$ increase in structural zero probability;
\end{itemize}

The structural zero probability $\pi_{ij}$ is modeled using a logstic function
\begin{align*}
    \text{logit}(\pi_{ij}) = \gamma_0 + \gamma_1 Z_{a1, ij} + \gamma_2 Z_{a2, ij}
\end{align*}

\subsubsection{Zero-inflated Binary GLMM}
There are some students who fail the course because they never take the test.


\[
P(Y_{ij} = y) = 
\begin{cases}
\pi_{ij} + (1 - \pi_{ij})(1 - p_{ij}), & \text{if } y = 0 \\
(1 - \pi_{ij}) p_{ij}, & \text{if } y = 1
\end{cases}
\]

\subsubsection{Zero-inflated Poisson GLMM}

There are some students who do not complete any module due to sickness and bad internet.

\[
P(Y = y) = 
\begin{cases}
\pi_{ij} + (1 - \pi_{ij})e^{-\lambda_{ij}}, & \text{if } y = 0 \\
(1 - \pi_{ij}) \dfrac{e^{-\lambda_{ij}} \lambda_{ij}^y}{y!}, & \text{if } y > 0
\end{cases}
\]





\begin{itemize}
    \item A unit decrease in network quality leads to an approximate $15\%$ increase in structural zero probability;
\end{itemize}

The structural zero probability $\pi_{ij}$ is modeled using a logstic function
\begin{align*}
    \text{logit}(\pi_{ij}) = \gamma_0 + \gamma_1 Z_{a1, ij} + \gamma_2 Z_{a2, ij}
\end{align*}

\section{Simultion Result}

\subsection{Result without Zero-inlfation}



This analysis conducts 500 simulations to assess the performance of Poisson and Binomial GLMMs under both small (\texttt{size = 5}) and large (\texttt{size = 50}) sample sizes. Nakagawa $R^2$ is obtained using the \texttt{performance} package, whereas Raudenbush $R^2$, PCV, McFadden $R^2$, and Non-converges check are computed manually in batch according to their analytical definitions.

\subsubsection{General Analysis of Different Metrics by Model}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{plot_dir/box.png}
    \caption{Boxplots of McFadden $R^2$, Nakagawa $R^2$, PCV, and Raudenbush $R^2$ Across Binary and Poisson GLMMs with Varying Cluster Sizes}
    \label{box}
\end{figure}

Based on the Figure \ref{box}, the following discussion are made:
\begin{itemize}
    \item \textbf{McFadden $R^2$:} In \textbf{Poisson GLMMs}, the McFadden $R^2$ values always remain high (between 0.75 and 1), indicating that the full models show substantial improvement in log-likelihood over the null models. In contrast, \textbf{Binomial GLMMs} yield significantly lower $R^2$ values ( between 0.1 and 0.3), which is consistent with the nature of binary response data and the inherent upper bound of McFadden’s $R^2$ in such settings. At smaller cluster sizes, $R^2$ values for both model types become more concentrated and slightly higher, suggesting that McFadden’s $R^2$ is more stable and model fit improves with smaller clusters. This pattern is particularly evident in Poisson GLMMs. These results collectively indicate that Poisson GLMMs demonstrate higher predictive efficiency and better model fit under the current data-generating mechanism.

    \item \textbf{Nakagawa $R^2$:} The results show a clear contrast between model types. In \textbf{Poisson GLMMs}, the \texttt{marginal $R^2$} values are substantially high (between 0.65 - 0.8), indicating that fixed effects explain a large portion of the variance. \texttt{Conditional $R^2$} values are consistently near 1.0, suggesting that the combination of fixed and random effects nearly fully explains the model variance. In contrast, \textbf{Binomial GLMMs} exhibit much lower \texttt{marginal $R^2$} values (between 0.3 and 0.4), which shows the relatively limited explanatory power of fixed effects in binary response data. However, the corresponding \texttt{conditional $R^2$} values are slightly higher, implying that a considerable portion of the explained variance arises from random effects such as cluster-level variation. Additionally, models with smaller cluster sizes tend to produce more concentrated $R^2$ values. This suggests the improvement of model stability under such configurations.

    \item \textbf{Non Converges:} Convergence status is monitored across all simulations to assess model stability. Non-convergence occurs only a few times (basically 8 times), within the large-sample Binomial GLMMs, indicating that estimation procedures across all four model types remain stable under the current simulation design.

    \item \textbf{PCV:} The PCV results exhibit extremely large negative values in the \texttt{logit\_small} model. This behavior is likely caused by the near-zero variance of the random effect in the null model, which leads to numerical instability in the PCV calculation. As a result, the boxplots for other models are visually compressed and difficult to interpret on the same scale. In contrast, the McFadden $R^2$ metric mitigates this issue by computing a log-likelihood ratio rather than a raw variance ratio. This transformation avoids numerical divergence when the denominator is small, making McFadden $R^2$ a more robust alternative for assessing model explanatory power, particularly when comparing full and null models.

    \item \textbf{Raudenbush $R^2$:} This value reveals clear differences between model types. \texttt{Conditional $R^2$} values for Poisson GLMMs approach 1.0, indicating that nearly all variance is accounted for by the combination of fixed and random effects. In contrast, Binomial GLMMs exhibit substantially lower \textbf{conditional $R^2$} values, particularly in the large-cluster setting. \texttt{Marginal $R^2$} values highlight a similar pattern. Poisson models demonstrate moderate to high fixed-effect explanatory power (median $\approx$ 0.4--0.5), whereas Binomial models show near-zero \texttt{marginal $R^2$} values, suggesting that most of their explanatory power stems from random effects rather than covariates. These results support the conclusion that Poisson GLMMs, under the given data-generating conditions, are more effective in capturing variance structure.

\end{itemize}


\subsubsection{Comparing Raudenbush and Nakagawa $R^2$}
\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plot_dir/Rplot01.png}
        \caption{Correlation Between Nakagawa and Raudenbush Conditional $R^2$ Across GLMM Structures}
    \end{minipage}
    \hfill
     \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plot_dir/N M vs R M.png}
        \caption{Correlation Between Nakagawa and Raudenbush Marginal $R^2$ Across GLMM Structures}
    \end{minipage}
    \caption{Comparing Raudenbush and Nakagawa $R^2$}
    \label{1}
\end{figure}

Figure \ref{1} displays the pairwise comparisons between Nakagawa and Raudenbush $R^2$ measures for both marginal and conditional definitions. \texttt{Conditional $R^2$} values show a nearly perfect linear relationship across all model types, indicating that the two formulations agree strongly in quantifying the combined explanatory power of fixed and random effects.

In contrast, the comparison of \texttt{marginal $R^2$} reveals substantial divergence, particularly within Poisson GLMMs. While the two metrics remain consistent in Binomial GLMMs, \texttt{Raudenbush marginal $R^2$} exhibits greater variability and more outliers in the Poisson case. This suggests that Raudenbush's formulation may be more sensitive to distributional skewness or model-specific variance structures, and should be interpreted with caution when used in conjunction with Poisson models.


\subsubsection{Summary of Fit Metric Distributions}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{plot_dir/P1.png}
    \caption{Distribution of Fit Metrics by Model}
    \label{hist}
\end{figure}

Figure \ref{hist} presents the distribution of 3 fit metrics across four GLMM structures, allowing for a direct visual comparison of metric behavior and stability. It shows that Poisson GLMMs consistently achieve higher values across all fit metrics, with \texttt{McFadden} and \texttt{Conditional $R^2$} values clustering near 1.0. In contrast, Binary GLMMs yield lower and more dispersed metrics, particularly in \texttt{marginal $R^2$} measures. These results suggest that under the current data-generating mechanism, Poisson models offer superior explanatory power and model fit stability.



\begin{table}[ht]
\centering
\caption{GLMM Model Summary Statistics}
\label{tab:glmm_r2}
\begin{tabular}{llcccc}
\toprule
\textbf{Model} & \textbf{Metric} & \textbf{Mean} & \textbf{SD} & \textbf{CI Lower} & \textbf{CI Upper} \\
\midrule
\multirow{6}{*}{logit\_large} 
  & McFaddenR2                  & 0.2051 & 0.0117 & 0.1842 & 0.2294 \\
  & Nakagawa\_Conditional\_R2   & 0.4774 & 0.0220 & 0.4355 & 0.5167 \\
  & Nakagawa\_Marginal\_R2      & 0.3208 & 0.0146 & 0.2833 & 0.3618 \\
  & PCV                         & -0.6766 & 0.1068 & -0.9055 & -0.4731 \\
  & Raudenbush\_Conditional\_R2 & 0.3134 & 0.0179 & 0.2795 & 0.3475 \\
  & Raudenbush\_Marginal\_R2    & 0.1804 & 0.0139 & 0.1573 & 0.2076 \\
\midrule
\multirow{6}{*}{logit\_small} 
  & McFaddenR2                  & 0.0845 & 0.0226 & 0.0563 & 0.1252 \\
  & Nakagawa\_Conditional\_R2   & 0.4648 & 0.2721 & 0.3259 & 0.6184 \\
  & Nakagawa\_Marginal\_R2      & 0.3268 & 0.0525 & 0.2682 & 0.4258 \\
  & PCV                         & -Inf & --- & -6.0768 & 1.1123 \\
  & Raudenbush\_Conditional\_R2 & 0.3605 & 0.0705 & 0.2182 & 0.5033 \\
  & Raudenbush\_Marginal\_R2    & 0.1831 & 0.0832 & 0.1099 & 0.2609 \\
\midrule
\multirow{6}{*}{pois\_large} 
  & McFaddenR2                  & 0.9254 & 0.0108 & 0.8998 & 0.9527 \\
  & Nakagawa\_Conditional\_R2   & 0.9900 & 0.0160 & 0.9920 & 0.9998 \\
  & Nakagawa\_Marginal\_R2      & 0.6605 & 0.0307 & 0.6004 & 0.7172 \\
  & PCV                         & 0.0828 & 0.0538 & -0.0247 & 0.1909 \\
  & Raudenbush\_Conditional\_R2 & 0.9778 & 0.0100 & 0.9955 & 0.9995 \\
  & Raudenbush\_Marginal\_R2    & 0.3281 & 0.1040 & 0.1243 & 0.5193 \\
\midrule
\multirow{6}{*}{pois\_small} 
  & McFaddenR2                  & 0.8744 & 0.0288 & 0.8159 & 0.9228 \\
  & Nakagawa\_Conditional\_R2   & 0.9896 & 0.0023 & 0.9948 & 0.9932 \\
  & Nakagawa\_Marginal\_R2      & 0.6614 & 0.0358 & 0.5914 & 0.7297 \\
  & PCV                         & 0.3701 & 0.0884 & 0.1924 & 0.5128 \\
  & Raudenbush\_Conditional\_R2 & 0.9979 & 0.0013 & 0.9945 & 0.9998 \\
  & Raudenbush\_Marginal\_R2    & 0.3498 & 0.1558 & 0.1185 & 0.6541 \\
\bottomrule
\end{tabular}
\label{table}
\end{table}

Based on the table \ref{table}, the following result can be presented:
\begin{itemize}
    
\item In Binary GLMMs, smaller cluster sizes slightly improve the fixed-effect explanatory power, as reflected in higher \texttt{Raudenbush marginal and Raudenbush conditional $R^2$} values. This effect likely arises from the increased influence of individual-level variation relative to cluster-level random effects in smaller groups. However, PCV calculations become unstable due to near-zero random effect variances in null models. Also, 
\texttt{Nakagawa conditional $R^2$} shows greater variability in the small-cluster setting, suggesting increased sensitivity to sampling noise when within-cluster counts are low.

\item For Poisson GLMMs, cluster size has limited impact on overall model fit, as both small and large cluster designs yield high $R^2$ values across all metrics. 

\end{itemize}



\section{Conlcusion}
This study offers a comprehensive evaluation of $R^2$ measures in generalized linear mixed models (GLMMs), focusing on Binary and Poisson GLMMs under two types of cluster size (small =5, large = 50). Through 500 times simulation-based analysis, several fit indices are compared, including McFadden's $R^2$, Nakagawa's marginal and conditional $R^2$, Raudenbush's marginal and conditional $R^2$, the Non-converges check, and the Proportional Change in Variance (PCV).

Poisson GLMMs consistently achieve higher fit quality across all metrics, with conditional $R^2$ values approaching 1.0 and marginal values exceeding those observed in binary models. In contrast, binary GLMMs exhibit lower fixed-effect explanatory power and greater numerical instability. The PCV results exhibit extremely large negative values in the \texttt{logit\_small} model. Therefore, it is a good choice to use a more stable metric, McFadden, with similar evaluation content to replace PCV.

Cluster size has a small impact in binary models, where smaller clusters yield more concentrated and slightly elevated $R^2$ values. This pattern is attributed to the increased influence of individual-level variation relative to between-cluster differences. The McFadden's $R^2$ changes more significantly with different sizes, especially in Pois GLMMs. $R^2$ become more concentrated and slightly higher, suggesting that McFadden’s $R^2$ is more stable and model fit improves with smaller clusters

Comparison of Raudenbush and Nakagawa measures shows near-identical conditional $R^2$ values, supporting consistency in quantifying combined variance explained by fixed and random effects. However, marginal $R^2$ values from the two frameworks diverge more substantially, particularly under Poisson GLMMs, indicating  Raudenbush is more sensitive to distributional assumptions and variance decomposition approaches.

Overall, Nakagawa’s conditional $R^2$ provides a stable and interpretable index for GLMM fit evaluation, especially when fixed and random effects are both of substantive interest. Careful selection of $R^2$ metrics while taking into account model structure, outcome distribution, and cluster design is essential for meaningful interpretation in applied multilevel modeling.

Future research will explore more complex simulation designs by incorporating group-level covariates into the fixed-effect structure and increasing the number of random-effect components.



\nocite{*}
\bibliographystyle{apalike}  
\bibliography{ref}


\clearpage
\appendix
\begin{center}
{\LARGE \textbf{Appendix}}  
\end{center}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\renewcommand{\thetable}{\thesection.\arabic{table}}

\section{Software}
R code for simulation method/ result can be found in https://github.com/zhili617/STAT-538-Final-Project

\section{Extra tables}
\begin{table}[ht]
    \centering
    \caption{Multiplicative Overdispersion Model}
    \label{tab:simulated_bf_mom}
    \begin{threeparttable}
        \begin{tabular}{@{}p{0.5\linewidth} p{0.5\linewidth}@{}}
            \toprule
            \multicolumn{1}{c}{Binomial Data} & \multicolumn{1}{c}{Count Data} \\
            \midrule
            a & b \\
            \begin{minipage}[t]{\linewidth}
    $Y_{ij} \sim \text{Overdispersed Binomial}(m_{ij}, p_{ij}, \omega)$ \\
    $\log\left(\frac{p_{ij}}{1 - p_{ij}}\right) = \beta_0 + \alpha_i$ \\
    $\alpha_i \sim N(0, \sigma^2_{\alpha})$
            \end{minipage} & 
            \begin{minipage}[t]{\linewidth}
    $Y_{ij} \sim \text{Overdispersed Poisson}(\mu_{ij}, \omega)$ \\
    $\mu_{ij}=exp(\beta_0 + \alpha_i)$ \\
    $\alpha_i \sim N(0, \sigma^2_{\alpha})$
            \end{minipage} \\
            & \\
            \begin{minipage}[t]{\linewidth}
                $Y_{ij}$: For the $i^{th}$ individual in the $j^{th}$ group, number of successes in $m_{ij}$ trials. \\
                For binary data, $m_{ij}=1$. \\
                $\alpha_i$: Random effect with variance $\sigma_{\alpha}^2$. \\
                $\omega$: Overdispersion parameter. \\
            \end{minipage}
            &
            \begin{minipage}[t]{\linewidth}
                $Y_{ij}$: For the $i^{th}$ individual in the $j^{th}$ group, the observed number of counts. \\
                $\mu_i$: Latent mean for the $i^{th}$ individual in the $j^{th}$ group. \\
                $\alpha_i$: Random effect with variance $\sigma_{\alpha}^2$. \\
                $\omega$: Overdispersion parameter. 
            \end{minipage} \\
            \bottomrule
        \end{tabular}
    \end{threeparttable}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Additive Overdispersion Model}
    \label{tab:simulated_bf_aom}
    \begin{threeparttable}
        \begin{tabular}{@{}p{0.5\linewidth} p{0.5\linewidth}@{}}
            \toprule
            \multicolumn{1}{c}{Binomial Data} & \multicolumn{1}{c}{Count Data} \\
            \midrule
            a & b \\
            \begin{minipage}[t]{\linewidth}
    $Y_{ij} \sim \text{Binomial}(m_{ij}, p_{ij})$ \\
    $\log\left(\frac{p_{ij}}{1 - p_{ij}}\right) = \beta_0 + \alpha_i + e_{ij}$ \\
    $\alpha_i \sim N(0, \sigma^2_{\alpha})$ \\
    $e_{ij} \sim N(0, \sigma_{e}^2)$
            \end{minipage} & 
            \begin{minipage}[t]{\linewidth}
    $Y_{ij} \sim \text{Poisson}(\mu_{ij})$ \\
    $\mu_{ij}=exp(\beta_0 + \alpha_i + e_{ij})$ \\
    $\alpha_i \sim N(0, \sigma^2_{\alpha})$ \\
    $e_{ij} \sim N(0, \sigma_{e}^2)$
            \end{minipage} \\
            & \\
            \begin{minipage}[t]{\linewidth}
                $Y_{ij}$: For the $i^{th}$ individual in the $j^{th}$ group, number of successes in $m_{ij}$ trials. \\
                For binary data, $m_{ij}=1$. \\
                $\alpha_i$: Random effect with variance $\sigma_{\alpha}^2$. \\
                $e_{ij}$: additive overdispersion (residual) term on the link scale, with a variance of $\sigma_e^2$. When $\sigma_e^2=0$, the model reduces to the normal binomial GLMM. 
            \end{minipage}
            &
            \begin{minipage}[t]{\linewidth}
                $Y_{ij}$: For the $i^{th}$ individual in the $j^{th}$ group, the observed number of counts. \\
                $\mu_i$: Latent mean for the $i^{th}$ individual in the $j^{th}$ group. \\
                $\alpha_i$: Random effect with variance $\sigma_{\alpha}^2$. \\
                $e_{ij}$: additive overdispersion (residual) term on the link scale, with a variance of $\sigma_e^2$. When $\sigma_e^2=0$, the model reduces to the normal Poisson GLMM. 
            \end{minipage} \\
            \bottomrule
        \end{tabular}
    \end{threeparttable}
\end{table}


\begin{sidewaystable}
    \centering
    \caption{Marginal and Conditional R-squared for GLMM: Multiplicative Overdispersion Model}
    \label{tab:simulated_bf_multiplicative}
    \begin{threeparttable}
        \begin{tabular}{@{}p{0.15\linewidth} p{0.4\linewidth} p{0.4\linewidth}@{}}
            \toprule
            \multicolumn{1}{c}{Data} & \multicolumn{1}{c}{Marginal $R^2$} & \multicolumn{1}{c}{Conditional $R^2$} \\
            \midrule
            & $R^2_{GLMM(m)} = \frac{\sigma^2_f}{\sigma_{f}^2 + \sum_{l=1}^u \sigma_l^2 + \omega \sigma_d^2}$ \newline
              $\sigma_{\epsilon}^2 = \omega \sigma_d^2$
            & $R^2_{GLMM(c)} = \frac{\sigma^2_f + \sum_{l=1}^u \sigma_l^2}{\sigma_{f}^2 + \sum_{l=1}^u \sigma_l^2 + \omega \sigma_d^2}$ \newline
              $\sigma_{\epsilon}^2 = \omega \sigma_d^2$ \\
            \addlinespace
            & \multicolumn{2}{p{\dimexpr0.85\linewidth\relax}}{
                $\sigma_{f}^2$: Fixed effects variance. \newline
                $\sigma_{l}^2$: $j^{th}$ random factor variance. \newline
                $\sigma_{d}^2$: Distribution-specific variance.
            } \\
            \addlinespace
            Binomial Data & \multicolumn{2}{l}{$\sigma_d^2 = \pi^2/3$} \\
            Count Data & \multicolumn{2}{l}{$\sigma_d^2 = \ln (1/\lambda + 1)$} \\
            \bottomrule
        \end{tabular}
    \end{threeparttable}
\end{sidewaystable}

\begin{sidewaystable}
    \centering
    \caption{Marginal and Conditional R-squared for GLMM: Additive Overdispersion Model}
    \label{tab:simulated_bf_additive}
    \begin{threeparttable}
        \begin{tabular}{@{}p{0.15\linewidth} p{0.4\linewidth} p{0.4\linewidth}@{}}
            \toprule
            \multicolumn{1}{c}{Data} & \multicolumn{1}{c}{Marginal $R^2$} & \multicolumn{1}{c}{Conditional $R^2$} \\
            \midrule
            & $R^2_{GLMM(m)} = \frac{\sigma^2_f}{\sigma_{f}^2 + \sum_{l=1}^u \sigma_l^2 + \sigma_e^2 + \sigma_d^2}$ \newline
              $\sigma_{\epsilon}^2 = \sigma_e^2 + \sigma_d^2$
            & $R^2_{GLMM(c)} = \frac{\sigma^2_f + \sum_{l=1}^u \sigma_l^2}{\sigma_{f}^2 + \sum_{l=1}^u \sigma_l^2 + \sigma_e^2 + \sigma_d^2}$ \newline
              $\sigma_{\epsilon}^2 = \sigma_e^2 + \sigma_d^2$ \\
            \addlinespace
            & \multicolumn{2}{p{\dimexpr0.85\linewidth\relax}}{
                $\sigma_{f}^2$: Variance explained by the fixed effect components. \newline
                $\sigma_{l}^2$: Variance of the $j^{th}$ random factor. \newline
                $\sigma_{e}^2$: Additive overdispersion residual variance. \newline
                $\sigma_{d}^2$: Distribution-specific variance.
            } \\
            \addlinespace
            Binomial Data & \multicolumn{2}{l}{$\sigma_d^2 = \pi^2/3$} \\
            Count Data & \multicolumn{2}{l}{$\sigma_d^2 = \ln (1/\lambda + 1)$} \\
            \bottomrule
        \end{tabular}
    \end{threeparttable}
\end{sidewaystable}





\setcounter{figure}{0}  % Reset figure counter
\setcounter{table}{0}   % Reset table counter




\end{document}
